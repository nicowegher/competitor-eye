# Scraper de Tarifas de Hoteles en Booking.com - Versi√≥n Mejorada

## üöÄ Descripci√≥n

Backend en Python para hacer scraping de tarifas de hoteles en Booking.com usando Apify. Esta versi√≥n incluye mejoras significativas para resolver problemas de timeout y memoria en Render.com.

## ‚ú® Mejoras Implementadas

- ‚úÖ **Ejecuci√≥n as√≠ncrona**: No m√°s timeouts de HTTP
- ‚úÖ **Configuraci√≥n optimizada**: Gunicorn configurado para Render
- ‚úÖ **Logging detallado**: Mejor visibilidad del progreso
- ‚úÖ **Manejo de errores robusto**: Recuperaci√≥n autom√°tica de fallos
- ‚úÖ **Estado en tiempo real**: Endpoint para monitorear progreso

## üõ†Ô∏è Tecnolog√≠as

- **Backend**: Flask + Gunicorn
- **Scraping**: Apify Client
- **Base de datos**: Firebase Firestore
- **Almacenamiento**: Google Cloud Storage
- **Deploy**: Render.com

## üìã Requisitos

- Python 3.8+
- Cuenta de Apify con token API
- Proyecto Firebase configurado
- Bucket de Google Cloud Storage

## üîß Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone <tu-repositorio>
cd scraper-hotel-tarifas
```

2. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

3. **Configurar variables de entorno**
```bash
# Crear archivo .env
FIREBASE_SERVICE_ACCOUNT={"tu-json-de-firebase"}
GCS_BUCKET_NAME=tu-bucket-name
APIFY_API_TOKEN=tu-token-de-apify
```

4. **Configurar Firebase**
- Crear proyecto en Firebase Console
- Descargar archivo de credenciales
- Configurar Firestore y Storage

## üöÄ Uso

### Endpoints Disponibles

#### 1. Health Check
```bash
GET /
```
Verifica que el backend est√© funcionando.

#### 2. Iniciar Scraper (NUEVO - As√≠ncrono)
```bash
POST /run-scraper
Content-Type: application/json

{
  "ownHotelUrl": "https://www.booking.com/hotel/ar/tu-hotel.es.html",
  "competitorHotelUrls": [
    "https://www.booking.com/hotel/ar/competidor1.es.html",
    "https://www.booking.com/hotel/ar/competidor2.es.html"
  ],
  "daysToScrape": 2
}
```

**Respuesta inmediata:**
```json
{
  "status": "started",
  "message": "Scraper iniciado correctamente",
  "hotels": 3,
  "days": 2
}
```

#### 3. Verificar Estado del Scraper (NUEVO)
```bash
GET /scraper-status
```

**Respuesta:**
```json
{
  "is_running": true,
  "progress": 45,
  "total_tasks": 6,
  "completed_tasks": 3,
  "error": null,
  "result": null
}
```

#### 4. Descargar Datos
```bash
GET /datos
GET /descargar-csv
GET /descargar-excel
```

## üîÑ Flujo de Uso Mejorado

1. **Iniciar scraper**
   ```bash
   curl -X POST https://tu-app.onrender.com/run-scraper \
     -H "Content-Type: application/json" \
     -d '{"ownHotelUrl": "...", "competitorHotelUrls": [...], "daysToScrape": 2}'
   ```

2. **Monitorear progreso**
   ```bash
   curl https://tu-app.onrender.com/scraper-status
   ```

3. **Verificar resultados en Firebase**
   - Los archivos se guardan autom√°ticamente en GCS
   - Las URLs se actualizan en Firestore
   - El estado se actualiza cuando completa

## üß™ Pruebas

Ejecutar el script de pruebas:
```bash
python test_scraper.py
```

**Nota**: Cambiar `BASE_URL` en el script por tu URL de Render.

## üìä Configuraci√≥n de Render

### Variables de Entorno Requeridas
- `FIREBASE_SERVICE_ACCOUNT`: JSON de credenciales de Firebase
- `GCS_BUCKET_NAME`: Nombre del bucket de Google Cloud Storage
- `APIFY_API_TOKEN`: Token de API de Apify

### Configuraci√≥n Autom√°tica
El `Procfile` usa la configuraci√≥n optimizada:
```
web: gunicorn app:app --config gunicorn.conf.py
```

## üîç Monitoreo y Logs

### Logs de Aplicaci√≥n
- Logs detallados en Render Dashboard
- Informaci√≥n de progreso del scraper
- Errores y excepciones capturadas

### Estado del Scraper
- Endpoint `/scraper-status` para monitoreo
- Estado en tiempo real en Firebase
- URLs de archivos generados autom√°ticamente

## üö® Soluci√≥n de Problemas

### Error: Worker Timeout
- ‚úÖ **Resuelto**: Configuraci√≥n de Gunicorn con timeout de 5 minutos
- ‚úÖ **Resuelto**: Ejecuci√≥n as√≠ncrona del scraper

### Error: Out of Memory
- ‚úÖ **Resuelto**: Configuraci√≥n optimizada de workers
- ‚úÖ **Resuelto**: Limpieza autom√°tica de archivos temporales

### Error: Scraper no responde
- ‚úÖ **Resuelto**: Endpoint de status para monitoreo
- ‚úÖ **Resuelto**: Logging detallado del progreso

## üìà Pr√≥ximas Mejoras

1. **Sistema de colas**: Para m√∫ltiples scrapers simult√°neos
2. **Webhooks**: Notificaciones autom√°ticas al frontend
3. **Rate limiting**: Protecci√≥n contra spam
4. **Caching**: Cachear resultados repetidos
5. **Dashboard**: Interfaz web para monitoreo

## üìù Notas Importantes

- El scraper de Apify funciona correctamente (seg√∫n los logs)
- Los problemas eran de configuraci√≥n del backend
- La nueva versi√≥n es compatible con el frontend existente
- Los archivos se guardan autom√°ticamente en GCS y Firebase

## ü§ù Contribuci√≥n

1. Fork el proyecto
2. Crear rama para feature (`git checkout -b feature/AmazingFeature`)
3. Commit cambios (`git commit -m 'Add AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abrir Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver `LICENSE` para m√°s detalles. 